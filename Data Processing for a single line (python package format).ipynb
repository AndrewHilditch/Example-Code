{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import random as rm\n",
    "import numpy as np\n",
    "import time\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process raw timetable data into a more manageable dataframe\n",
    "def Process_raw_timetable(Timetablepath,Timetablefilename):\n",
    "    #Read in timetable data\n",
    "    data_folder = Timetablepath\n",
    "    filename = data_folder+ Timetablefilename\n",
    "    data = pd.read_csv(filename, header=None, names=['tag'])\n",
    "    data['Msg_type'] = data['tag'].str.slice(0,2)\n",
    "    data = data[data['Msg_type']!='AA'].reset_index(drop=True)\n",
    "    #Process basic schedule data type\n",
    "    BSdata= data[data['Msg_type']=='BS'].copy()\n",
    "    BSdata['TransactionType'] = BSdata['tag'].str.slice(2,3)\n",
    "    BSdata['TrainUID'] = BSdata['tag'].str.slice(3,9)\n",
    "    BSdata['Datefrom'] = BSdata['tag'].str.slice(9,15)\n",
    "    BSdata['Dateto'] = BSdata['tag'].str.slice(15,21)\n",
    "    BSdata['Daysrun'] = BSdata['tag'].str.slice(21,28)\n",
    "    BSdata['Bankhol'] = BSdata['tag'].str.slice(28,29)\n",
    "    BSdata['Trainstatus'] = BSdata['tag'].str.slice(29,30)\n",
    "    BSdata['Traincategory'] = BSdata['tag'].str.slice(30,32)\n",
    "    BSdata['Trainidentity'] = BSdata['tag'].str.slice(32,36)\n",
    "    BSdata['Headcode'] = BSdata['tag'].str.slice(36,40)\n",
    "    BSdata['Servicecode'] = BSdata['tag'].str.slice(41,49)\n",
    "    BSdata['Speed'] = BSdata['tag'].str.slice(57,60)\n",
    "    BSdata['Operatechar'] = BSdata['tag'].str.slice(60,66)\n",
    "    BSdata['STP']=BSdata['tag'].str.slice(79,80)\n",
    "    del BSdata['tag']\n",
    "    BSdata=BSdata.reset_index()\n",
    "    STPC=BSdata[BSdata['STP']=='C'].reset_index(drop=True)\n",
    "    for i in range(0,len(BSdata)-1):\n",
    "        if BSdata['index'][i]==BSdata['index'][i+1]-1:\n",
    "            BSdata=BSdata.drop([i])\n",
    "    BSdata=BSdata.reset_index(drop=True)\n",
    "    #Process train origin data type\n",
    "    LOdata= data[data['Msg_type']=='LO'].copy()\n",
    "    LOdata['Location'] = LOdata['tag'].str.slice(2,10)\n",
    "    LOdata['SchedArr'] = LOdata['tag'].str.slice(50,51)\n",
    "    LOdata['SchedDept'] = LOdata['tag'].str.slice(10,15)\n",
    "    LOdata['SchedPass'] = LOdata['tag'].str.slice(50,51)\n",
    "    LOdata['PubArr'] = LOdata['tag'].str.slice(50,51)\n",
    "    LOdata['PubDept'] = LOdata['tag'].str.slice(15,19)\n",
    "    LOdata['Platform'] = LOdata['tag'].str.slice(19,22)\n",
    "    LOdata['Line'] = LOdata['tag'].str.slice(22,25)\n",
    "    LOdata['Path'] = LOdata['tag'].str.slice(50,51)\n",
    "    LOdata['Activity'] = LOdata['tag'].str.slice(29,41)\n",
    "    LOdata['Engallow'] = LOdata['tag'].str.slice(25,27)\n",
    "    LOdata['Pathallow'] = LOdata['tag'].str.slice(27,29)\n",
    "    LOdata['Perfallow'] = LOdata['tag'].str.slice(41,43)\n",
    "    del LOdata['tag']\n",
    "    #Process intermediate train movements data type\n",
    "    LIdata= data[data['Msg_type']=='LI'].copy()\n",
    "    LIdata['Location'] = LIdata['tag'].str.slice(2,10)\n",
    "    LIdata['SchedArr'] = LIdata['tag'].str.slice(10,15)\n",
    "    LIdata['SchedDept'] = LIdata['tag'].str.slice(15,20)\n",
    "    LIdata['SchedPass'] = LIdata['tag'].str.slice(20,25)\n",
    "    LIdata['PubArr'] = LIdata['tag'].str.slice(25,29)\n",
    "    LIdata['PubDept'] = LIdata['tag'].str.slice(29,33)\n",
    "    LIdata['Platform'] = LIdata['tag'].str.slice(33,36)\n",
    "    LIdata['Line'] = LIdata['tag'].str.slice(36,39)\n",
    "    LIdata['Path'] = LIdata['tag'].str.slice(39,42)\n",
    "    LIdata['Activity'] = LIdata['tag'].str.slice(40,54)\n",
    "    LIdata['Engallow'] = LIdata['tag'].str.slice(54,56)\n",
    "    LIdata['Pathallow'] = LIdata['tag'].str.slice(56,58)\n",
    "    LIdata['Perfallow'] = LIdata['tag'].str.slice(58,60)\n",
    "    del LIdata['tag']\n",
    "    #Process train terminal movements data type\n",
    "    LTdata= data[data['Msg_type']=='LT'].copy()\n",
    "    LTdata['Location'] = LTdata['tag'].str.slice(2,10)\n",
    "    LTdata['SchedArr'] = LTdata['tag'].str.slice(10,15)\n",
    "    LTdata['SchedDept'] = LTdata['tag'].str.slice(50,51)\n",
    "    LTdata['SchedPass'] = LTdata['tag'].str.slice(50,51)\n",
    "    LTdata['PubArr'] = LTdata['tag'].str.slice(15,19)\n",
    "    LTdata['PubDept'] = LTdata['tag'].str.slice(50,51)\n",
    "    LTdata['Platform'] = LTdata['tag'].str.slice(19,22)\n",
    "    LTdata['Line'] = LTdata['tag'].str.slice(50,51)\n",
    "    LTdata['Path'] = LTdata['tag'].str.slice(22,25)\n",
    "    LTdata['Activity'] = LTdata['tag'].str.slice(25,37)\n",
    "    LTdata['Engallow'] = LTdata['tag'].str.slice(50,51)\n",
    "    LTdata['Pathallow'] = LTdata['tag'].str.slice(50,51)\n",
    "    LTdata['Perfallow'] = LTdata['tag'].str.slice(50,51)\n",
    "    del LTdata['tag']\n",
    "    #Recombine these message types to get to the timetable\n",
    "    Timetabledata=LOdata.append(LIdata)\n",
    "    Timetabledata=Timetabledata.append(LTdata)\n",
    "    Timetabledata=Timetabledata.sort_index()\n",
    "    Timetabledata=Timetabledata.reset_index()\n",
    "    del Timetabledata['index']\n",
    "    #Add additional information from the basic schedule messages\n",
    "    k=0\n",
    "    Timetabledata['TrainUID']=Timetabledata['Location']\n",
    "    Timetabledata['Headcode']=Timetabledata['Location']\n",
    "    Timetabledata['Daysrun']=Timetabledata['Location']\n",
    "    Timetabledata['Datesrun']=Timetabledata['Location']\n",
    "    for i in range(0,len(Timetabledata)):\n",
    "        Timetabledata.at[i,'TrainUID']=BSdata['TrainUID'][k]\n",
    "        Timetabledata.at[i,'Headcode']=BSdata['Trainidentity'][k]\n",
    "        Timetabledata.at[i,'Daysrun']=BSdata['Daysrun'][k]\n",
    "        Timetabledata.at[i,'Datesrun']=BSdata['Datefrom'][k]+BSdata['Dateto'][k]\n",
    "        Timetabledata.at[i,'STP']=BSdata['STP'][k]\n",
    "        if Timetabledata['Msg_type'][i]=='LT':\n",
    "            k=k+1\n",
    "    Timetabledata['Datefrom']=pd.to_datetime('20'+Timetabledata['Datesrun'].str.slice(0,6),format='%Y%m%d')\n",
    "    Timetabledata['Dateto']=pd.to_datetime('20'+Timetabledata['Datesrun'].str.slice(6,12),format='%Y%m%d')\n",
    "    del Timetabledata['Datesrun']\n",
    "    return Timetabledata,STPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Timetabledata,STPC=Process_raw_timetable(\"/home/mathsys1/Documents/Train_Data/Data_from_Ian/\",\"POINTA20190930OU.LPF\")\n",
    "Timetabledata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Timetabledata[Timetabledata['Location']=='PITSEA  ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del STPC['Bankhol']\n",
    "del STPC['Trainstatus']\n",
    "del STPC['Traincategory']\n",
    "del STPC['Trainidentity']\n",
    "del STPC['Headcode']\n",
    "del STPC['Servicecode']\n",
    "del STPC['Speed']\n",
    "del STPC['Operatechar']\n",
    "STPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out=open(\"Alt2019-09-02Timetable.pickle\",\"wb\")\n",
    "#pickle.dump(Timetabledata, pickle_out)\n",
    "#pickle_out=open(\"2019-09-01STP.pickle\",\"wb\")\n",
    "#pickle.dump(Timetabledata['STP'], pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out=open(\"2019-09-30STPC.pickle\",\"wb\")\n",
    "pickle.dump(STPC, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read and process the STANOX-to-berth data effectively turning it into\n",
    "# TIPLOC-to-berth data\n",
    "def STANOXberth_processing(STANOXberthpath,STANOXberthfilename,TIPLOCSTANOXpath,TIPLOCSTANOXfilename):\n",
    "    #Read in the data\n",
    "    data_folder = STANOXberthpath\n",
    "    filename = data_folder+STANOXberthfilename\n",
    "    STANOXberth=pd.read_csv(filename)\n",
    "    #Also requires the STANOX-TIPLOC data\n",
    "    data_folder = TIPLOCSTANOXpath\n",
    "    filename = data_folder+TIPLOCSTANOXfilename\n",
    "    STANOXTIPLOC=pd.read_csv(filename)\n",
    "    #Append TIPLOCs to the STANOXberth dataframe\n",
    "    STANberth=STANOXberth.copy()\n",
    "    STANberth['TIPLOC']=str(STANberth['STANOX'])\n",
    "    for i in range(0,len(STANberth['STANOX'])):\n",
    "        Interim=STANOXTIPLOC[STANOXTIPLOC['STANOX']==str(STANberth['STANOX'][i])].reset_index()\n",
    "        if len(Interim)>0:\n",
    "            STANberth.at[i,'TIPLOC']=Interim['TIPLOC'][0]\n",
    "        else:\n",
    "            STANberth.at[i,'TIPLOC']='NaN'\n",
    "    \n",
    "    #Removes all ofsets except the standard 'B' type offset\n",
    "    STANberth=STANberth[STANberth['STEPTYPE']=='B'].reset_index(drop=True)\n",
    "    return STANberth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANberth=STANOXberth_processing(\"/home/mathsys1/Documents/\",\"berth_step_mapping_20191120.csv\",\"/home/mathsys1/Documents/\",\"TIPLOC-STANOX.csv\")\n",
    "STANberth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding trigger berths and offsets to the dataframe of timetable data\n",
    "def Process_cleaned_timetable(Timetablepath,Timetablefilename,STANOXberthpath,STANOXberthfilename,TIPLOCSTANOXpath,TIPLOCSTANOXfilename):\n",
    "    #Call previous functions to have prerequisites \n",
    "    Timetabledata,STPC=Process_raw_timetable(Timetablepath,Timetablefilename)\n",
    "    STANberth=STANOXberth_processing(STANOXberthpath,STANOXberthfilename,TIPLOCSTANOXpath,TIPLOCSTANOXfilename)\n",
    "    for i in range(0,len(Timetabledata)):\n",
    "        Timetabledata.at[i,'Location']=Timetabledata['Location'][i].strip()\n",
    "        for j in range(0,5):\n",
    "            Timetabledata.at[i,'Location']=Timetabledata['Location'][i].rstrip(str(j))\n",
    "    #Add berths and relevant offsets to the Timetabledata dataframe\n",
    "    Timetabledata['Berth']=np.array\n",
    "    Timetabledata['Offset']=np.array\n",
    "    my_list=[]\n",
    "    my_other_list=[]\n",
    "    for i in range(0,len(Timetabledata['Location'])):\n",
    "        prep=STANberth[STANberth['TIPLOC']==Timetabledata['Location'][i]].reset_index()\n",
    "        if len(prep['FROMBERTH'])>0:\n",
    "            for j in range(0,len(prep['FROMBERTH'])):\n",
    "                tupl=list([prep['FROMBERTH'][j],prep['TOBERTH'][j]])\n",
    "                my_list=my_list+[tupl]\n",
    "                my_other_list=my_other_list+[prep['STOPPINGOFFSET'][j]]\n",
    "            Timetabledata.at[i,'Berth']=my_list\n",
    "            Timetabledata.at[i,'Offset']=my_other_list\n",
    "            my_list=[]\n",
    "            my_other_list=[]\n",
    "        else:\n",
    "            Timetabledata.at[i,'Berth']='NaN'\n",
    "            Timetabledata.at[i,'Offset']='NaN'\n",
    "    return Timetabledata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Timetabledata=Process_cleaned_timetable(\"/home/mathsys1/Documents/Train_Data/Data_from_Ian/\",\"POINTA20190930OU.LPF\",\"/home/mathsys1/Documents/\",\"berth_step_mapping_20191120.csv\",\"/home/mathsys1/Documents/\",\"TIPLOC-STANOX.csv\")\n",
    "Timetabledata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/STANberth.pickle\"\n",
    "pickle_in=open(filename,\"rb\")\n",
    "STANberth=pickle.load(pickle_in)\n",
    "STANberth1=STANberth[STANberth['TD_ID']=='UR'].reset_index(drop=True)\n",
    "#STANberth2=STANberth[STANberth['TD_ID']=='U2'].reset_index(drop=True)\n",
    "a=STANberth1['TIPLOC'].unique()\n",
    "#b=STANberth2['TIPLOC'].unique()\n",
    "#a=np.append(a,b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(Timetabledata)):\n",
    "    if np.isin(Timetabledata['Location'][i][0:7],a)==True:\n",
    "        Timetabledata.at[i,'Path']='UR'\n",
    "    #if np.isin(Timetabledata['Location'][i][0:7],b)==True:\n",
    "    #    Timetabledata.at[i,'Path']='U2'\n",
    "Timetabledata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Timetabledata[Timetabledata['Location']=='PITSEA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Your=Timetabledata[Timetabledata['Path']=='UR'].reset_index(drop=True)\n",
    "#You2=Timetabledata[Timetabledata['Path']=='U2'].reset_index(drop=True)\n",
    "RelData=Your.copy()#append(You2).reset_index(drop=True)\n",
    "RelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out=open(\"Alt2019-09-30Timetable.pickle\",\"wb\")\n",
    "pickle.dump(RelData, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename=\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Alt2019-09-01Timetable.pickle\"\n",
    "pickle_in=open(filename,\"rb\")\n",
    "Time=pickle.load(pickle_in)\n",
    "Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Time[Time['Location']=='PITSEA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RelData['Daysrun']=RelData['STP']+RelData['Daysrun']\n",
    "#Export=RelData.to_csv(\"/home/mathsys1/Documents/Alt2019-09-01Timetable.csv\",index=None,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selects a day of data based on inputs, calls previous functions\n",
    "#to get data so takes a while to run\n",
    "def Day_iso(Dayinput,Timetablepath,Timetablefilename,STANOXberthpath,STANOXberthfilename,TIPLOCSTANOXpath,TIPLOCSTANOXfilename):\n",
    "    Timetabledata=Process_cleaned_timetable(Timetablepath,Timetablefilename,STANOXberthpath,STANOXberthfilename,TIPLOCSTANOXpath,TIPLOCSTANOXfilename)\n",
    "    WedTimetable=Timetabledata.copy()\n",
    "    #Dayinput format is 'YYYY-MM-DD 00:00:00'\n",
    "    Date=pd.Timestamp(Dayinput, freq='D')\n",
    "    WedTimetable=WedTimetable[WedTimetable['Date']==Date]\n",
    "    WedTimetable=WedTimetable.reset_index(drop=True)\n",
    "    return WedTimetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selectdayschedule=Day_iso(\"2019-09-01 00:00:00\",\"/home/mathsys1/Documents/Train_Data/Data_from_Ian/\",\"POINTA20190901LF.csv\",\"/home/mathsys1/Documents/\",\"berth_step_mapping_20191120.csv\",\"/home/mathsys1/Documents/\",\"TIPLOC-STANOX.csv\")\n",
    "Selectdayschedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out=open(\"New2019-09-01Timetable.pickle\",\"wb\")\n",
    "pickle.dump(Timetabledata, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to clean and process berth data\n",
    "def Berth_data_processing(Dayinput,Berthpath,Berthfilename,AreacodeA,AreacodeB):\n",
    "    #Read in the data\n",
    "    data_folder = Berthpath\n",
    "    filename = data_folder+Berthfilename\n",
    "    Berthdata=pd.read_csv(filename,names=['tag'])\n",
    "    #Split the data into dataframe columns\n",
    "    Berthdata['UTC']=Berthdata['tag'].str.slice(15,21)\n",
    "    Berthdata['tag']=Berthdata['tag'].str.slice(61,77)\n",
    "    Berthdata['Area']=Berthdata['tag'].str.slice(0,2)\n",
    "    Berthdata['msg_type']=Berthdata['tag'].str.slice(2,4)\n",
    "    Berthdata['From'] = Berthdata['tag'].str.slice(4,8)\n",
    "    Berthdata['To'] = Berthdata['tag'].str.slice(8,12)\n",
    "    Berthdata['Headcode'] = Berthdata['tag'].str.slice(12,16)\n",
    "    del Berthdata['tag']\n",
    "    #Converts the UTC column into a datetime object \n",
    "    Berthdata['RealUTC']=pd.to_datetime(Dayinput+Berthdata['UTC'],format='%Y%m%d%H%M%S')\n",
    "    for i in range(0,len(Berthdata)):\n",
    "        if int(Berthdata['UTC'][i])<40000:\n",
    "            Berthdata.at[i,'RealUTC']=Berthdata['RealUTC'][i]+timedelta(days=1)\n",
    "    #Filter the data futher selecting area code and message type\n",
    "    #also retaining only the passenger trains\n",
    "    BerthdataA=Berthdata[Berthdata['Area']==AreacodeA]\n",
    "    #BerthdataB=Berthdata[Berthdata['Area']==AreacodeB]\n",
    "    Berthdata=BerthdataA.copy()#.append(BerthdataB)\n",
    "    Train=Berthdata[Berthdata['msg_type']=='CA']\n",
    "    #pattern =r'[1-2][A-Z][0-9][0-9]'\n",
    "    #Train=Train[Train['Headcode'].str.match(pattern)].reset_index(drop=True)\n",
    "    #Example code for using pickles to save dataframes\n",
    "    #pickle_out=open(\"filename.pickle\",\"wb\")\n",
    "    #pickle.dump(Train, pickle_out)\n",
    "    return Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UTC</th>\n",
       "      <th>Area</th>\n",
       "      <th>msg_type</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Headcode</th>\n",
       "      <th>RealUTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>040009</td>\n",
       "      <td>UR</td>\n",
       "      <td>CA</td>\n",
       "      <td>0611</td>\n",
       "      <td>0911</td>\n",
       "      <td>4L80</td>\n",
       "      <td>2019-10-05 04:00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>040048</td>\n",
       "      <td>UR</td>\n",
       "      <td>CA</td>\n",
       "      <td>0911</td>\n",
       "      <td>0529</td>\n",
       "      <td>4L80</td>\n",
       "      <td>2019-10-05 04:00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1486</td>\n",
       "      <td>040112</td>\n",
       "      <td>UR</td>\n",
       "      <td>CA</td>\n",
       "      <td>0610</td>\n",
       "      <td>0604</td>\n",
       "      <td>4M83</td>\n",
       "      <td>2019-10-05 04:01:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1788</td>\n",
       "      <td>040124</td>\n",
       "      <td>UR</td>\n",
       "      <td>CA</td>\n",
       "      <td>1053</td>\n",
       "      <td>R354</td>\n",
       "      <td>5B01</td>\n",
       "      <td>2019-10-05 04:01:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1983</td>\n",
       "      <td>040133</td>\n",
       "      <td>UR</td>\n",
       "      <td>CA</td>\n",
       "      <td>0529</td>\n",
       "      <td>0177</td>\n",
       "      <td>4L80</td>\n",
       "      <td>2019-10-05 04:01:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5961541</td>\n",
       "      <td>014407</td>\n",
       "      <td>UR</td>\n",
       "      <td>CA</td>\n",
       "      <td>0353</td>\n",
       "      <td>R354</td>\n",
       "      <td>2B78</td>\n",
       "      <td>2019-10-06 01:44:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5966582</td>\n",
       "      <td>014931</td>\n",
       "      <td>UR</td>\n",
       "      <td>CA</td>\n",
       "      <td>B592</td>\n",
       "      <td>A592</td>\n",
       "      <td>5S33</td>\n",
       "      <td>2019-10-06 01:49:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5967789</td>\n",
       "      <td>015052</td>\n",
       "      <td>UR</td>\n",
       "      <td>CA</td>\n",
       "      <td>A592</td>\n",
       "      <td>SBCE</td>\n",
       "      <td>5S33</td>\n",
       "      <td>2019-10-06 01:50:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6025662</td>\n",
       "      <td>030012</td>\n",
       "      <td>UR</td>\n",
       "      <td>CA</td>\n",
       "      <td>C354</td>\n",
       "      <td>A354</td>\n",
       "      <td>5S35</td>\n",
       "      <td>2019-10-06 03:00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6027966</td>\n",
       "      <td>030251</td>\n",
       "      <td>UR</td>\n",
       "      <td>CA</td>\n",
       "      <td>A354</td>\n",
       "      <td>SBCE</td>\n",
       "      <td>5S35</td>\n",
       "      <td>2019-10-06 03:02:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20069 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            UTC Area msg_type  From    To Headcode             RealUTC\n",
       "161      040009   UR       CA  0611  0911     4L80 2019-10-05 04:00:09\n",
       "1013     040048   UR       CA  0911  0529     4L80 2019-10-05 04:00:48\n",
       "1486     040112   UR       CA  0610  0604     4M83 2019-10-05 04:01:12\n",
       "1788     040124   UR       CA  1053  R354     5B01 2019-10-05 04:01:24\n",
       "1983     040133   UR       CA  0529  0177     4L80 2019-10-05 04:01:33\n",
       "...         ...  ...      ...   ...   ...      ...                 ...\n",
       "5961541  014407   UR       CA  0353  R354     2B78 2019-10-06 01:44:07\n",
       "5966582  014931   UR       CA  B592  A592     5S33 2019-10-06 01:49:31\n",
       "5967789  015052   UR       CA  A592  SBCE     5S33 2019-10-06 01:50:52\n",
       "6025662  030012   UR       CA  C354  A354     5S35 2019-10-06 03:00:12\n",
       "6027966  030251   UR       CA  A354  SBCE     5S35 2019-10-06 03:02:51\n",
       "\n",
       "[20069 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Berthdata=Berth_data_processing(\"20191005\",\"/home/mathsys1/Documents/Steve/\",\"NewTD_20191005.txt\",\"UR\",\"U2\")\n",
    "Berthdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Berthdata=Berthdata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out=open(\"2019-10-05Berth.pickle\",\"wb\")\n",
    "pickle.dump(Berthdata, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to read in a pickle data object \n",
    "def Pickle_read(pathtopickle):\n",
    "    #Read in the pickle\n",
    "    filename=pathtopickle\n",
    "    pickle_in=open(filename,\"rb\")\n",
    "    Train=pickle.load(pickle_in)\n",
    "    return Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pickle_read(\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Berth.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to calculate SRTs(Sectional running times) for the\n",
    "#berth data that is imported via pickles from the Berth_data_processing\n",
    "#function. The input is required to be in array format\n",
    "def SRT_calculation(picklepathsarray):\n",
    "    Shift=pd.DataFrame(columns=['UTC','Area','msg_type','From','To','Headcode','RealUTC','prev_from','prev_to','prev_RealUTC','prev_headcode'])\n",
    "    for i in range(0,len(picklepathsarray)):\n",
    "        Train=Pickle_read(picklepathsarray[i])\n",
    "        pattern =r'[2][A-Z][0-9][0-9]'\n",
    "        Train=Train[Train['Headcode'].str.match(pattern)]\n",
    "        grouped=Train.groupby('Headcode')\n",
    "        for code,group in grouped:\n",
    "            g=group.copy().sort_values(by='RealUTC',ascending=True)\n",
    "            g['prev_from']=g['From'].shift()\n",
    "            g['prev_to']=g['To'].shift()\n",
    "            g['prev_RealUTC']=g['RealUTC'].shift()\n",
    "            g['prev_headcode']=g['Headcode'].shift()\n",
    "            g=g.dropna()\n",
    "            Shift=Shift.append(g,sort=True)\n",
    "    Shift=Shift.reset_index(drop=True)\n",
    "    Shift['ID']=Shift['Area']+Shift['From']+Shift['To']\n",
    "    Shift['Time']=Shift['RealUTC']-Shift['prev_RealUTC']\n",
    "    #pattern =r'[0-9][0-9][0-9][0-9]'\n",
    "    #data = Shift[Shift['To'].str.match(pattern)]\n",
    "    #data = data[data['From'].str.match(pattern)]\n",
    "    #data = data[data['prev_from'].str.match(pattern)]\n",
    "    #data = data[data['prev_to'].str.match(pattern)]\n",
    "    SRT=pd.DataFrame(columns=['Area','From','To','Headcode','UTC','msg_type','prev_from','prev_to','prev_RealUTC','Median','STD','Mean'])\n",
    "    grouped=Shift.groupby('ID')\n",
    "    i=0\n",
    "    for code,group in grouped:\n",
    "        g=group.copy().reset_index(drop=True)\n",
    "        SRT=SRT.append(g.iloc[0]).reset_index(drop=True)\n",
    "        SRT.at[i,'Median']=g['Time'].median()\n",
    "        SRT.at[i,'STD']=g['Time'].std()\n",
    "        SRT.at[i,'Mean']=g['Time'].mean()\n",
    "        i=i+1\n",
    "    SRT=SRT.reset_index(drop=True)\n",
    "    #SRT['STD/Mean']=SRT['STD']/SRT['Mean']\n",
    "    #SRT['seconds']=SRT['Time']/np.timedelta64(1,'s')\n",
    "    return SRT,Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SRT,Shift=SRT_calculation([\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-02Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-03Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-04Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-05Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-06Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-09Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-10Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-11Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-12Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-13Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-16Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-17Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-18Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-19Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-20Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-23Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-24Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-25Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-26Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-27Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-09-30Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-02Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-03Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-04Berth.pickle\"])\n",
    "SRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out=open(\"SRT2.pickle\",\"wb\")\n",
    "pickle.dump(SRT, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dayta=Shift[Shift['ID']=='UR01120108'].reset_index(drop=True)\n",
    "dayta['Time'].astype('timedelta64[s]').plot.hist(bins=20)\n",
    "dayta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRT[SRT['ID']=='UR01120108']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_out=open(\"SRT.pickle\",\"wb\")\n",
    "#pickle.dump(SRT, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes in all the previously calculated berth and timetable\n",
    "#data along with the STANOX-berth-TIPLOC mapping and filters for\n",
    "#a single line \n",
    "def Further_timetable_processing(Timetablepickle,STANberthpickle,Areacode):\n",
    "    Timetable=Pickle_read(Timetablepickle)\n",
    "    STANberth=Pickle_read(STANberthpickle)\n",
    "    #Remove freight trains\n",
    "    pattern =r'[1-2][A-Z][0-9][0-9]'\n",
    "    Timetable=Timetable[Timetable['Headcode'].str.match(pattern)].reset_index(drop=True)\n",
    "    \n",
    "    #Only observe TIPLOCs from the chosen signalling area\n",
    "    STANberth1=STANberth[STANberth['TD_ID']==Areacode]\n",
    "    a=STANberth1['TIPLOC'].unique()\n",
    "    a=a[a!='NaN']\n",
    "    for i in range(0,len(Timetable)):\n",
    "        if np.isin(Timetable['Location'][i],a)==False:\n",
    "            Timetable=Timetable.drop(i)\n",
    "    Timetable=Timetable.reset_index(drop=True)\n",
    "    #Use the SchedPass column to fill the missing values in the SchedArr\n",
    "    #and SchedDept columns\n",
    "    for i in range(0,len(Timetable)):\n",
    "        if Timetable['SchedArr'][i]==' ':\n",
    "            Timetable.at[i,'SchedArr']=Timetable['SchedDept'][i]\n",
    "        if Timetable['SchedArr'][i]=='     ':\n",
    "            Timetable.at[i,'SchedArr']=Timetable['SchedPass'][i]\n",
    "        if Timetable['SchedDept'][i]==' ':\n",
    "            Timetable.at[i,'SchedDept']=Timetable['SchedArr'][i]\n",
    "        if Timetable['SchedDept'][i]=='     ':\n",
    "            Timetable.at[i,'SchedDept']=Timetable['SchedPass'][i]\n",
    "    #Converts the SchedArr and SchedDept into more useful datetime objects\n",
    "    \n",
    "    Timetable['ModTime']=Timetable['SchedArr'].str.slice(0,4)\n",
    "    Timetable['Half']=Timetable['SchedArr'].str.slice(4,5)\n",
    "    Timetable['Time']=pd.to_datetime('20191001'+Timetable['ModTime']+'00',format='%Y%m%d%H%M%S')\n",
    "    Timetable['ArrTime']=Timetable['Time']\n",
    "    for i in range(0,len(Timetable['Half'])):\n",
    "        if Timetable['Half'][i]=='H':\n",
    "            Timetable.at[i,'ArrTime']=Timetable['Time'][i]+timedelta(seconds=30)\n",
    "    Timetable['ModTime']=Timetable['SchedDept'].str.slice(0,4)\n",
    "    Timetable['Half']=Timetable['SchedDept'].str.slice(4,5)\n",
    "    Timetable['Time']=pd.to_datetime('20191001'+Timetable['ModTime']+'00',format='%Y%m%d%H%M%S')\n",
    "    Timetable['DeptTime']=Timetable['Time']\n",
    "    for i in range(0,len(Timetable['Half'])):\n",
    "        if Timetable['Half'][i]=='H':\n",
    "            Timetable.at[i,'DeptTime']=Timetable['Time'][i]+timedelta(seconds=30)\n",
    "    return Timetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=Further_timetable_processing(\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Timetable.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/STANberth.pickle\",\"UR\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that calculates the median travel time between TIPLOCs\n",
    "#for a chosen signalling area\n",
    "def Sched_TIPLOC_tt(Timetablepickle,STANberthpickle,Areacode):\n",
    "    Timetable=Further_timetable_processing(Timetablepickle,STANberthpickle,Areacode)\n",
    "    #Shift the dataframe columns allowing direct comparisons in each row\n",
    "    Shifty=pd.DataFrame()\n",
    "    grouped=Timetable.groupby('TrainUID')\n",
    "    for code,group in grouped:\n",
    "        g=group.copy()\n",
    "        g['prev_TIPLOC']=g['Location'].shift()\n",
    "        g['prev_DeptTime']=g['DeptTime'].shift()\n",
    "        Shifty=Shifty.append(g)\n",
    "    Shifty=Shifty.reset_index(drop=True)\n",
    "    #Calculate travel time\n",
    "    Shifty['TravelTime']=Shifty['ArrTime']-Shifty['prev_DeptTime']\n",
    "    Shifty=Shifty.dropna().reset_index(drop=True)\n",
    "    Shifty=Shifty[Shifty['Location']!=Shifty['prev_TIPLOC']].reset_index(drop=True)\n",
    "    Shifty['ID']=Shifty['prev_TIPLOC']+':'+Shifty['Location']\n",
    "    #Aggregate the travel times for each TIPLOC pair\n",
    "    SchedConnection=pd.DataFrame(columns=['prev_TIPLOC','Location','TravelTime','ID'])\n",
    "    grouped=Shifty.groupby('ID')\n",
    "    for code,group in grouped:\n",
    "        g=group.copy().reset_index(drop=True)\n",
    "        g.at[0,'TravelTime']=g['TravelTime'].median()\n",
    "        SchedConnection=SchedConnection.append(g.loc[0]).reset_index(drop=True)\n",
    "    SchedConnection['Seconds']=SchedConnection['TravelTime']/np.timedelta64(1,'s')\n",
    "    #Clean up the dataframe\n",
    "    del SchedConnection['Activity']\n",
    "    del SchedConnection['ArrTime']\n",
    "    del SchedConnection['Berth']\n",
    "    del SchedConnection['Offset']\n",
    "    del SchedConnection['Date']\n",
    "    del SchedConnection['Dept']\n",
    "    del SchedConnection['DeptTime']\n",
    "    del SchedConnection['Engallow']\n",
    "    del SchedConnection['Half']\n",
    "    del SchedConnection['Line']\n",
    "    del SchedConnection['Headcode']\n",
    "    del SchedConnection['ModTime']\n",
    "    del SchedConnection['Msg_type']\n",
    "    del SchedConnection['Path']\n",
    "    del SchedConnection['Pathallow']\n",
    "    del SchedConnection['Platform']\n",
    "    del SchedConnection['Perfallow']\n",
    "    del SchedConnection['PubArr']\n",
    "    del SchedConnection['PubDept']\n",
    "    del SchedConnection['SchedArr']\n",
    "    del SchedConnection['SchedDept']\n",
    "    del SchedConnection['SchedPass']\n",
    "    del SchedConnection['Time']\n",
    "    del SchedConnection['TrainUID']\n",
    "    del SchedConnection['prev_DeptTime']\n",
    "    return SchedConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SchedConnection=Sched_TIPLOC_tt(\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Timetable.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/STANberth.pickle\",\"UR\")\n",
    "SchedConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that adds the timetable to the berth movements\n",
    "def Berth_movements_appender(Berthpickle,Timetablepickle,STANberthpickle,Areacode):\n",
    "    #Timetable=Further_timetable_processing(Timetablepickle,STANberthpickle,Areacode)\n",
    "    Timetable=Pickle_read(Timetablepickle)\n",
    "    Train=Pickle_read(Berthpickle)\n",
    "    Train['TIPLOC']=Train['Area']\n",
    "    Train['Offset']=Train['Area']\n",
    "    Train['Sched']=Train['Area']\n",
    "    Headcode=Train['Headcode'].unique()\n",
    "    Headcode2=Timetable['Headcode'].unique()\n",
    "    Df=pd.DataFrame(columns=Train.columns)\n",
    "    #Create a set of headcodes that are present in both the timetable\n",
    "    #and the berth movements\n",
    "    Headmerge=np.empty(0)\n",
    "    k=0\n",
    "    for i in range(0,len(Headcode)):\n",
    "        for j in range(0,len(Headcode2)):\n",
    "            if Headcode[i]==Headcode2[j]:\n",
    "                k=k+1\n",
    "                Headmerge=np.append(Headmerge,Headcode[i])\n",
    "    #Iterates through all the headcodes and connects the berth movements\n",
    "    # to the timetable\n",
    "    for l in range(0,len(Headmerge)):\n",
    "        PrepTrain=Train[Train['Headcode']==Headmerge[l]].reset_index(drop=True)\n",
    "        PrepSched=Timetable[Timetable['Headcode']==Headmerge[l]].reset_index(drop=True)\n",
    "        for i in range(0,len(PrepTrain['TIPLOC'])):\n",
    "            for j in range(0,len(PrepSched['Location'])):\n",
    "                for k in range(0,len(PrepSched['Berth'][j])):\n",
    "                    if PrepTrain['From'][i]==PrepSched['Berth'][j][k][0] and PrepTrain['To'][i]==PrepSched['Berth'][j][k][1]:\n",
    "                        PrepTrain.at[i,'TIPLOC']=PrepSched['Location'][j]\n",
    "                        PrepTrain.at[i,'Offset']=int(PrepSched['Offset'][j][k])\n",
    "                        PrepTrain.at[i,'Sched']=PrepSched['Time'][j]\n",
    "        Df=Df.append(PrepTrain)\n",
    "    Df=Df.reset_index(drop=True)\n",
    "    Df['Delay']=Df['Area']\n",
    "    for i in range(0,len(Df['Offset'])):\n",
    "        if Df['Offset'][i]=='UR' or Df['Offset'][i]=='U2':\n",
    "            Df.at[i,'Offset']=0\n",
    "    Df['NewUTC']=Df['RealUTC']\n",
    "    for i in range(0,len(Df['NewUTC'])):\n",
    "        Df.at[i,'AdjUTC']=Df['NewUTC'][i]+timedelta(seconds=int(Df['Offset'][i]))\n",
    "        if Df['Sched'][i]=='UR' or Df['Sched'][i]=='U2':\n",
    "            Df.at[i,'Sched']=Df['AdjUTC'][i]\n",
    "        Df.at[i,'Delay']=Df['AdjUTC'][i]-Df['Sched'][i]\n",
    "    del Df['RealUTC']\n",
    "    del Df['msg_type']\n",
    "    del Df['UTC']\n",
    "    return Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Headcode</th>\n",
       "      <th>TIPLOC</th>\n",
       "      <th>Offset</th>\n",
       "      <th>Sched</th>\n",
       "      <th>Delay</th>\n",
       "      <th>NewUTC</th>\n",
       "      <th>AdjUTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>UR</td>\n",
       "      <td>0667</td>\n",
       "      <td>0669</td>\n",
       "      <td>4L80</td>\n",
       "      <td>UR</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-01 04:00:33</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>2019-10-01 04:00:33</td>\n",
       "      <td>2019-10-01 04:00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>UR</td>\n",
       "      <td>0669</td>\n",
       "      <td>0671</td>\n",
       "      <td>4L80</td>\n",
       "      <td>UR</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-01 04:01:15</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>2019-10-01 04:01:15</td>\n",
       "      <td>2019-10-01 04:01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>UR</td>\n",
       "      <td>0671</td>\n",
       "      <td>0673</td>\n",
       "      <td>4L80</td>\n",
       "      <td>GRAYS</td>\n",
       "      <td>51</td>\n",
       "      <td>2019-10-01 04:09:30</td>\n",
       "      <td>-1 days +23:53:28</td>\n",
       "      <td>2019-10-01 04:02:07</td>\n",
       "      <td>2019-10-01 04:02:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>UR</td>\n",
       "      <td>0673</td>\n",
       "      <td>0675</td>\n",
       "      <td>4L80</td>\n",
       "      <td>GRAYS</td>\n",
       "      <td>-10</td>\n",
       "      <td>2019-10-01 04:09:30</td>\n",
       "      <td>-1 days +23:53:08</td>\n",
       "      <td>2019-10-01 04:02:48</td>\n",
       "      <td>2019-10-01 04:02:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>UR</td>\n",
       "      <td>0885</td>\n",
       "      <td>LG11</td>\n",
       "      <td>4L80</td>\n",
       "      <td>UR</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-01 04:18:36</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>2019-10-01 04:18:36</td>\n",
       "      <td>2019-10-01 04:18:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31451</td>\n",
       "      <td>UR</td>\n",
       "      <td>0610</td>\n",
       "      <td>0604</td>\n",
       "      <td>4M83</td>\n",
       "      <td>BARKING</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-01 03:35:00</td>\n",
       "      <td>0 days 23:49:07</td>\n",
       "      <td>2019-10-02 03:24:07</td>\n",
       "      <td>2019-10-02 03:24:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31452</td>\n",
       "      <td>UR</td>\n",
       "      <td>0604</td>\n",
       "      <td>L924</td>\n",
       "      <td>4M83</td>\n",
       "      <td>UR</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-02 03:25:12</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>2019-10-02 03:25:12</td>\n",
       "      <td>2019-10-02 03:25:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31453</td>\n",
       "      <td>UR</td>\n",
       "      <td>L924</td>\n",
       "      <td>L920</td>\n",
       "      <td>4M83</td>\n",
       "      <td>UR</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-02 03:26:09</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>2019-10-02 03:26:09</td>\n",
       "      <td>2019-10-02 03:26:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31454</td>\n",
       "      <td>UR</td>\n",
       "      <td>B920</td>\n",
       "      <td>LS21</td>\n",
       "      <td>4M83</td>\n",
       "      <td>UR</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-02 03:26:47</td>\n",
       "      <td>0 days 00:00:00</td>\n",
       "      <td>2019-10-02 03:26:47</td>\n",
       "      <td>2019-10-02 03:26:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31455</td>\n",
       "      <td>UR</td>\n",
       "      <td>1055</td>\n",
       "      <td>R596</td>\n",
       "      <td>5B51</td>\n",
       "      <td>SHBRYNS</td>\n",
       "      <td>98</td>\n",
       "      <td>2019-10-01 04:00:00</td>\n",
       "      <td>1 days 00:01:02</td>\n",
       "      <td>2019-10-02 03:59:24</td>\n",
       "      <td>2019-10-02 04:01:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31456 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Area  From    To Headcode   TIPLOC Offset                Sched  \\\n",
       "0       UR  0667  0669     4L80       UR      0  2019-10-01 04:00:33   \n",
       "1       UR  0669  0671     4L80       UR      0  2019-10-01 04:01:15   \n",
       "2       UR  0671  0673     4L80    GRAYS     51  2019-10-01 04:09:30   \n",
       "3       UR  0673  0675     4L80    GRAYS    -10  2019-10-01 04:09:30   \n",
       "4       UR  0885  LG11     4L80       UR      0  2019-10-01 04:18:36   \n",
       "...    ...   ...   ...      ...      ...    ...                  ...   \n",
       "31451   UR  0610  0604     4M83  BARKING      0  2019-10-01 03:35:00   \n",
       "31452   UR  0604  L924     4M83       UR      0  2019-10-02 03:25:12   \n",
       "31453   UR  L924  L920     4M83       UR      0  2019-10-02 03:26:09   \n",
       "31454   UR  B920  LS21     4M83       UR      0  2019-10-02 03:26:47   \n",
       "31455   UR  1055  R596     5B51  SHBRYNS     98  2019-10-01 04:00:00   \n",
       "\n",
       "                   Delay              NewUTC              AdjUTC  \n",
       "0        0 days 00:00:00 2019-10-01 04:00:33 2019-10-01 04:00:33  \n",
       "1        0 days 00:00:00 2019-10-01 04:01:15 2019-10-01 04:01:15  \n",
       "2      -1 days +23:53:28 2019-10-01 04:02:07 2019-10-01 04:02:58  \n",
       "3      -1 days +23:53:08 2019-10-01 04:02:48 2019-10-01 04:02:38  \n",
       "4        0 days 00:00:00 2019-10-01 04:18:36 2019-10-01 04:18:36  \n",
       "...                  ...                 ...                 ...  \n",
       "31451    0 days 23:49:07 2019-10-02 03:24:07 2019-10-02 03:24:07  \n",
       "31452    0 days 00:00:00 2019-10-02 03:25:12 2019-10-02 03:25:12  \n",
       "31453    0 days 00:00:00 2019-10-02 03:26:09 2019-10-02 03:26:09  \n",
       "31454    0 days 00:00:00 2019-10-02 03:26:47 2019-10-02 03:26:47  \n",
       "31455    1 days 00:01:02 2019-10-02 03:59:24 2019-10-02 04:01:02  \n",
       "\n",
       "[31456 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Df=Berth_movements_appender(\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Timetable.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/STANberth.pickle\",\"UR\")\n",
    "Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out=open(\"20191001Df.pickle\",\"wb\")\n",
    "pickle.dump(Df, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AltTimetable=pd.DataFrame()\n",
    "grouped=PrepSched.groupby('TrainUID')\n",
    "for code,group in grouped:\n",
    "    g=group.copy().reset_index(drop=True)\n",
    "    g=g.sort_values(by=['Location'])\n",
    "    for i in range(0,len(g)-1):\n",
    "        if g['Location'][i]==g['Location'][i+1]:\n",
    "            if g['STP'][i]=='O':\n",
    "                g.drop([i+1])\n",
    "                g=g.reset_index(drop=True)\n",
    "                display(g)\n",
    "            elif g['STP'][i+1]=='O':\n",
    "                g.drop([i])\n",
    "                g=g.reset_index(drop=True)\n",
    "                display(g)\n",
    "    #if len(g)>=1:\n",
    "    #    AltTimetable=AltTimetable.append(g).reset_index(drop=True)\n",
    "#AltTimetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PrepSched.sort_values(by=['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out=open(\"20190930Df.pickle\",\"wb\")\n",
    "pickle.dump(Df, pickle_out)\n",
    "#Dfa=Pickle_read(\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/20190901Df.pickle\")\n",
    "#Dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates a measure of how delayed a train is\n",
    "def Delay_calculation(Berthpickle,Timetablepickle,STANberthpickle,Areacode):\n",
    "    Df=Berth_movements_appender(Berthpickle,Timetablepickle,STANberthpickle,Areacode)\n",
    "    Df['Delay']=Df['Area']\n",
    "    for i in range(0,len(Df['Offset'])):\n",
    "        if Df['Offset'][i]=='UR':\n",
    "            Df.at[i,'Offset']=0\n",
    "    Df['NewUTC']=Df['RealUTC']\n",
    "    for i in range(0,len(Df['NewUTC'])):\n",
    "        Df.at[i,'AdjUTC']=Df['NewUTC'][i]+timedelta(seconds=Df['Offset'][i])\n",
    "        if Df['Sched'][i]=='UR':\n",
    "            Df.at[i,'Sched']=Df['AdjUTC'][i]\n",
    "        Df.at[i,'Delay']=Df['AdjUTC'][i]-Df['Sched'][i]\n",
    "    del Df['RealUTC']\n",
    "    del Df['msg_type']\n",
    "    del Df['UTC']\n",
    "    return Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Df=Delay_calculation(\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Timetable.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/STANberth.pickle\",\"UR\")\n",
    "Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the travel time between TIPLOCs from detection to detection\n",
    "def Actual_TIPLOC_tt(Berthpickle,Timetablepickle,STANberthpickle,Areacode):\n",
    "    Df=Delay_calculation(Berthpickle,Timetablepickle,STANberthpickle,Areacode)\n",
    "    AltDf=Df[Df['TIPLOC']!='UR']\n",
    "    #Calculate travel times between TIPLOCs\n",
    "    Shift=pd.DataFrame(columns=['Area','From','To','Headcode','UTC','msg_type','prev_TIPLOC','prev_AdjUTC','prev_NewUTC'])\n",
    "    grouped=AltDf.groupby('Headcode')\n",
    "    for code,group in grouped:\n",
    "        g=group.copy()\n",
    "        g['prev_TIPLOC']=g['TIPLOC'].shift()\n",
    "        g['prev_AdjUTC']=g['AdjUTC'].shift()\n",
    "        g['prev_NewUTC']=g['NewUTC'].shift()\n",
    "        Shift=Shift.append(g)\n",
    "    Shift=Shift.reset_index(drop=True)\n",
    "    Shift['TravelTime']=Shift['NewUTC']-Shift['prev_NewUTC']\n",
    "    altdf=pd.DataFrame(columns=['prev_TIPLOC','TIPLOC','TravelTime'])\n",
    "    altdf['TIPLOC']=Shift['TIPLOC']\n",
    "    altdf['prev_TIPLOC']=Shift['prev_TIPLOC']\n",
    "    altdf['TravelTime']=Shift['TravelTime']\n",
    "    altdf=altdf[altdf['prev_TIPLOC']!=altdf['TIPLOC']].reset_index(drop=True)\n",
    "    altdf['Connection']=altdf['prev_TIPLOC']+':'+altdf['TIPLOC']\n",
    "    #Create the new dataframe to store these new values\n",
    "    Connection=pd.DataFrame(columns=['prev_TIPLOC','TIPLOC','TravelTime','Connection'])\n",
    "    grouped=altdf.groupby('Connection')\n",
    "    for code,group in grouped:\n",
    "        g=group.copy().reset_index(drop=True)\n",
    "        g.at[0,'TravelTime']=g['TravelTime'].mean()\n",
    "        Connection=Connection.append(g.loc[0])\n",
    "    Connection=Connection.reset_index(drop=True)\n",
    "    Connection['seconds']=Connection['TravelTime']/np.timedelta64(1,'s')\n",
    "    return Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Connection=Actual_TIPLOC_tt(\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Timetable.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/STANberth.pickle\",\"UR\")\n",
    "Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate predicted arrival time at the next TIPLOC predicting ahead\n",
    "#based on the median SRTs between TIPLOCs\n",
    "def Predict_future_arrival(picklepathsarray,Berthpickle,Timetablepickle,STANberthpickle,Areacode):\n",
    "    #Read in and process data\n",
    "    BerthSRT=SRT_calculation(picklepathsarray)\n",
    "    Df=Delay_calculation(Berthpickle,Timetablepickle,STANberthpickle,Areacode)\n",
    "    del BerthSRT['Headcode']\n",
    "    del BerthSRT['msg_type']\n",
    "    del BerthSRT['UTC']\n",
    "    del BerthSRT['RealUTC']\n",
    "    del BerthSRT['prev_RealUTC']\n",
    "    del BerthSRT['prev_from']\n",
    "    del BerthSRT['prev_to']\n",
    "    #Shift the data to allow travel times to be calculated\n",
    "    Shifted=pd.DataFrame(columns=['Area','From','To','Headcode','TIPLOC','Offset','Sched','Delay','NewUTC','AdjUTC'])\n",
    "    grouped=Df.groupby('Headcode')\n",
    "    for code,group in grouped:\n",
    "        g=group.copy()\n",
    "        g['prev_AdjUTC']=g['AdjUTC'].shift()\n",
    "        Shifted=Shifted.append(g)\n",
    "    Shifted=Shifted.reset_index(drop=True)\n",
    "    Shifted['TravelTime']=Shifted['AdjUTC']-Shifted['prev_AdjUTC']\n",
    "    Shifted['ID']=Shifted['Area']+Shifted['From']+Shifted['To']\n",
    "    #Add the median SRT travel time\n",
    "    Shifted['MedTT']=Shifted['TravelTime']\n",
    "    for i in range(0,len(Shifted)):\n",
    "        for j in range(0,len(BerthSRT)):\n",
    "            if Shifted['ID'][i]==BerthSRT['ID'][j]:\n",
    "                Shifted.at[i,'MedTT']=BerthSRT['Median'][j]\n",
    "                break\n",
    "    #Calculate the difference in the travel times allowing use to\n",
    "    #say if the train lost or gained time in that berth section\n",
    "    Shifted['TTdiff']=Shifted['TravelTime']-Shifted['MedTT']\n",
    "    for i in range(1,len(Shifted)):\n",
    "        if Shifted['TIPLOC'][i]=='UR':\n",
    "            Shifted.at[i,'Delay']=Shifted['Delay'][i-1]+Shifted['TTdiff'][i]\n",
    "    Shifted=Shifted.dropna().reset_index(drop=True)\n",
    "    #Project ahead to calculate the estimated arrival time at the next TIPLOC\n",
    "    a=np.timedelta64(0,'s')\n",
    "    b=1\n",
    "    Shifted['PredArr']=Shifted['TIPLOC']\n",
    "    for i in range(0,len(Shifted)-1):\n",
    "        if Shifted['TIPLOC'][i]!='UR':\n",
    "            while Shifted['TIPLOC'][i+b]=='UR':\n",
    "                if Shifted['TIPLOC'][i+b]=='UR':\n",
    "                    a=a+Shifted['MedTT'][i+b]\n",
    "                    b=b+1\n",
    "            a=a+Shifted['MedTT'][i+b]\n",
    "            Shifted.at[i,'PredArr']=Shifted['AdjUTC'][i]+a\n",
    "            a=np.timedelta64(0,'s')\n",
    "        b=1\n",
    "    return Shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shifted=Predict_future_arrival([\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-02Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-03Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-04Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-05Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-06Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-07Berth.pickle\"],\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Timetable.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/STANberth.pickle\",\"UR\")\n",
    "Shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function calculates the travel time between TIPLOCs by adding\n",
    "#up the median travel times for the intermediate berths\n",
    "def Berth_level_tt(picklepathsarray,Berthpickle,Timetablepickle,STANberthpickle,Areacode):\n",
    "    #This code uses the previously calulated dataframe from the Predict_future_arrival\n",
    "    #function so runs it to get the dataframe\n",
    "    Shifted=Predict_future_arrival(picklepathsarray,Berthpickle,Timetablepickle,STANberthpickle,Areacode)\n",
    "    #Prepare various dataframes and counting variables\n",
    "    TIPLOCtt=pd.DataFrame(columns=['From','To','tt'])\n",
    "    a=np.timedelta64(0,'s')\n",
    "    b=1\n",
    "    c=0\n",
    "    #Run an adapted version of the previous function to count berth times\n",
    "    grouped=Shifted.groupby('Headcode')\n",
    "    for code,group in grouped:\n",
    "        g=group.copy().reset_index(drop=True)\n",
    "        if g['TIPLOC'][len(g)-1]=='UR':\n",
    "            g.at[len(g),'TIPLOC']='U2'\n",
    "        for i in range(0,len(g)-1):\n",
    "            if g['TIPLOC'][i]!='UR':\n",
    "                TIPLOCtt.at[c,'From']=g['TIPLOC'][i]\n",
    "                while g['TIPLOC'][i+b]=='UR':\n",
    "                    if g['TIPLOC'][i+b]=='UR':\n",
    "                        a=a+g['MedTT'][i+b]\n",
    "                        b=b+1\n",
    "                a=a+g['MedTT'][i+b]\n",
    "                TIPLOCtt.at[c,'To']=g['TIPLOC'][i+b]\n",
    "                TIPLOCtt.at[c,'tt']=a\n",
    "                c=c+1\n",
    "                a=np.timedelta64(0,'s')\n",
    "            b=1\n",
    "    TIPLOCtt=TIPLOCtt[TIPLOCtt['From']!=TIPLOCtt['To']].reset_index(drop=True)\n",
    "    TIPLOCtt=TIPLOCtt[TIPLOCtt['To']!='U2'].reset_index(drop=True)\n",
    "    TIPLOCtt['Connection']=TIPLOCtt['From']+':'+TIPLOCtt['To']\n",
    "    TIPLOCtt['seconds']=TIPLOCtt['Connection']\n",
    "    for i in range(0,len(TIPLOCtt)):\n",
    "        TIPLOCtt.at[i,'seconds']=TIPLOCtt['tt'][i]/np.timedelta64(1,'s')\n",
    "    AltConnection=pd.DataFrame(columns=['From','To','tt','Connection'])\n",
    "    grouped=TIPLOCtt.groupby('Connection')\n",
    "    for code,group in grouped:\n",
    "        g=group.copy().reset_index(drop=True)\n",
    "        g.at[0,'seconds']=g['seconds'].mean()\n",
    "        AltConnection=AltConnection.append(g.loc[0]).reset_index(drop=True)\n",
    "    return AltConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AltConnection=Berth_level_tt([\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-02Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-03Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-04Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-05Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-06Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-07Berth.pickle\"],\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Timetable.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/STANberth.pickle\",\"UR\")\n",
    "AltConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compare_Connection(picklepathsarray,Berthpickle,Timetablepickle,STANberthpickle,Areacode):\n",
    "    Connection=Actual_TIPLOC_tt(Berthpickle,Timetablepickle,STANberthpickle,Areacode)\n",
    "    SchedConnection=Sched_TIPLOC_tt(Timetablepickle,STANberthpickle,Areacode)\n",
    "    AltConnection=Berth_level_tt(picklepathsarray,Berthpickle,Timetablepickle,STANberthpickle,Areacode)\n",
    "    SumConnection=pd.DataFrame(columns=['From','To','ID'])\n",
    "    a=Connection['Connection']\n",
    "    a=a.append(SchedConnection['ID'])\n",
    "    a=a.append(AltConnection['Connection'])\n",
    "    a=a.unique()\n",
    "    SumConnection['ID']=a\n",
    "    SumConnection['From']=SumConnection['ID'].str.slice(0,7)\n",
    "    SumConnection['To']=SumConnection['ID'].str.slice(7,14)\n",
    "    SumConnection['TIPLOCseconds']=SumConnection['From']\n",
    "    SumConnection['Berthseconds']=SumConnection['From']\n",
    "    SumConnection['Schedseconds']=SumConnection['From']\n",
    "    for i in range(0,len(SumConnection)):\n",
    "        for j in range(0,len(Connection)):\n",
    "            if SumConnection['ID'][i]==Connection['Connection'][j]:\n",
    "                SumConnection.at[i,'TIPLOCseconds']=Connection['seconds'][j]\n",
    "        for k in range(0,len(AltConnection)):\n",
    "            if SumConnection['ID'][i]==AltConnection['Connection'][k]:\n",
    "                SumConnection.at[i,'Berthseconds']=AltConnection['seconds'][k]\n",
    "        for l in range(0,len(SchedConnection)):\n",
    "            if SumConnection['ID'][i]==SchedConnection['ID'][l]:\n",
    "                SumConnection.at[i,'Schedseconds']=SchedConnection['Seconds'][l]\n",
    "    for i in range(0,len(SumConnection)):\n",
    "        x=SumConnection['ID'][i].split(\":\")\n",
    "        SumConnection.at[i,'From']=x[0]\n",
    "        SumConnection.at[i,'To']=x[1]\n",
    "    return SumConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SumConnection=Compare_Connection([\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-02Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-03Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-04Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-05Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-06Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-07Berth.pickle\"],\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Berth.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/Data_pickles/2019-10-01Timetable.pickle\",\"/home/mathsys1/Documents/MathSys/Network_rail_data_analysis/STANberth.pickle\",\"UR\")\n",
    "SumConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
